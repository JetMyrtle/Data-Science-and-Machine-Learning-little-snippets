{"cells":[{"metadata":{},"cell_type":"markdown","source":"We are trying to execute this in two different approaches : one for statistical and one for machine learning\nThat could also be split in two : one using lifetimes models and one combined with regression approach\n\nI - The modeling & evaluation process is going to be the following:\n\n 1.Fit and evaluate BG/NBD model for frequency prediction\n 2.Fit and evaluate Gamma-Gamma model for monetary value  prediction\n 3.Combine 2 models into CLV model and compare to baseline\n 4. Refit the model on the entire dataset\n \n \n The statistical and traditional ML Model is based on calibration perdio : features period and holdout period : target period\n Each customer has two functions : possibility of buy at time t , and possibility to churn at time t"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install lifetimes\n!pip install scikit-learn\n!pip install keras\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n#  Dataframes and arrays processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime \n\n# to make the notebook reproductible\nnp.random.seed(42)\nimport random\nrandom.seed(42)\n\nimport warnings \nwarnings.filterwarnings('ignore')\nimport lifetimes\n\n\n# Statistical LTV (lifetime value)\nimport lifetimes\nfrom lifetimes import BetaGeoFitter, GammaGammaFitter\nfrom lifetimes.utils import calibration_and_holdout_data, summary_data_from_transaction_data\n\n\n# Plotting \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1st approach in this notebook we are using Statistical model with Lifetimes (Python library)\nStatistical approach : models BG/NBD and Gamma/Gamma\nCalibration and holdout : features and target periods => unsupervised learning"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Make default parameters bigger\nplt.rcParams['figure.figsize'] = (7,4.5) \nplt.rcParams[\"figure.dpi\"] = 140 \n\nsns.set(style=\"ticks\")\nsns.set_context(\"poster\", font_scale = .5, rc={\"grid.linewidth\": 5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Read the datasets bits by bits : the transaction logs : daily amount spent by each customer each day\ndf1 = pd.read_csv('../input/brazilian-ecommerce/olist_orders_dataset.csv')\ndf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.read_csv('../input/brazilian-ecommerce/olist_customers_dataset.csv')\ndf2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = pd.read_csv('../input/brazilian-ecommerce/olist_order_payments_dataset.csv')\ndf3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['customer_id','order_id','order_purchase_timestamp']\norders = df1[cols]\norders = orders.set_index('customer_id')\norders.drop_duplicates (inplace =True)\n\ncols =['customer_id','customer_unique_id']\ncustomers = df2 [cols]\ncustomers = customers.set_index('customer_id')\n\ncols = ['order_id','payment_value']\npayment = df3[cols]\npayment = payment.set_index('order_id')\npayment.drop_duplicates(inplace = True)\n\n\n# elog is for scraping\nelog = pd.concat([orders,customers], axis=1, join='inner')\nelog.reset_index(inplace=True)\n                 \n                 \ncols = [ 'customer_unique_id','order_purchase_timestamp']\nelog = elog[cols]\n\n\n# Datetime transformation                 \nelog['order_purchase_timestamp'] = pd.to_datetime(elog['order_purchase_timestamp'])\nelog['order_date'] = elog.order_purchase_timestamp.dt.date\nelog['order_date'] = pd.to_datetime(elog['order_date'])\n\n                 \ncols = ['customer_unique_id', 'order_date']\nelog = elog [cols]\n                 \nelog.columns = ['CUSTOMER_ID','ORDER_DATE']\n\nelog.info()\ndisplay(elog.sample(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Date range of orders\nelog.ORDER_DATE.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Creating RFM (Recency, Frequency, Monetary value) based on transaction log\n# Splitting calibration and holdout period"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n%%time\ncalibration_period_ends = '2018-06-30'\n\nfrom lifetimes.utils import calibration_and_holdout_data\n\nsummary_cal_holdout = calibration_and_holdout_data(elog, \n                                                   customer_id_col = 'CUSTOMER_ID', \n                                                   datetime_col = 'ORDER_DATE', \n                                                   freq = 'D', #days\n                                        calibration_period_end = '2017-08-27',\n                                        observation_period_end ='2018-09-28' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature set \nsummary_cal_holdout.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the Model MBG (Modifiedbetageofitter)\nThe Number of  transactions follow the Poisson process with transaction rate lambda"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n%%time\n\nfrom lifetimes import ModifiedBetaGeoFitter\n\nmbgnbd = ModifiedBetaGeoFitter(penalizer_coef=0.01)\nmbgnbd.fit(summary_cal_holdout['frequency_cal'], \n        summary_cal_holdout['recency_cal'], \n        summary_cal_holdout['T_cal'],\n       verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mbgnbd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Predicting for each customer\nt = 90 # days to predict in the future \nsummary_cal_holdout['predicted_purchases'] = mbgnbd.conditional_expected_number_of_purchases_up_to_time(t, \n                                                                                      summary_cal_holdout['frequency_cal'], \n                                                                                      summary_cal_holdout['recency_cal'], \n                                                                                      summary_cal_holdout['T_cal'])\n\nsummary_cal_holdout['p_alive'] = mbgnbd.conditional_probability_alive(summary_cal_holdout['frequency_cal'], \n                                                                         summary_cal_holdout['recency_cal'], \n                                                                         summary_cal_holdout['T_cal'])\nsummary_cal_holdout['p_alive'] = np.round(summary_cal_holdout['p_alive'] / summary_cal_holdout['p_alive'].max(), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndisplay(summary_cal_holdout.sample(2).T)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best to Worst customers "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ranking customers from Best to Worst (the more likely to purchase to more likely to churn)\nt = 1\nsummary_cal_holdout ['predicted_purchases'] = mbgnbd.conditional_expected_number_of_purchases_up_to_time(t,summary_cal_holdout['frequency_cal'],summary_cal_holdout['recency_cal'], summary_cal_holdout ['T_cal'])\nsummary_cal_holdout.sort_values (by= 'predicted_purchases'). tail(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Matrix recency and frequency\n\nfrom lifetimes.plotting import plot_frequency_recency_matrix\nplot_frequency_recency_matrix(mbgnbd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Probalility of still being alive\nfrom lifetimes.plotting import plot_probability_alive_matrix\nplot_probability_alive_matrix(mbgnbd)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model evaluation and assessment "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom lifetimes.plotting import plot_period_transactions\nax = plot_period_transactions(mbgnbd, max_frequency=7)\nax.set_yscale('log')\nsns.despine();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\nfrom lifetimes.plotting import plot_calibration_purchases_vs_holdout_purchases\n\nplot_calibration_purchases_vs_holdout_purchases(mbgnbd, summary_cal_holdout)\nsns.despine();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting an individual future purchase -- here (historical purchases )"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Customer Probability History\n\nfrom lifetimes.plotting import plot_history_alive\nfrom datetime import date\nfrom pylab import figure, text, scatter, show\n\nindividual = summary_cal_holdout.iloc[4942]\n\nid = individual.name\nt = 365*50\n\ntoday = date.today()\ntwo_year_ago = today.replace(year=today.year - 2)\none_year_from_now = today.replace(year=today.year + 1)\n\nsp_trans = elog.loc[elog['CUSTOMER_ID'] == id]\n\nfrom lifetimes.utils import calculate_alive_path\n\nt = (today - sp_trans.ORDER_DATE.min().date()).days\np_alive_today = pd.DataFrame(calculate_alive_path(mbgnbd, sp_trans, 'ORDER_DATE', t, freq='D'))[0].tail(1).values\np_alive_today = np.round(p_alive_today[0], 2)\nprint('Probability that customer is alive today is', p_alive_today)\n\nt = (one_year_from_now - sp_trans.ORDER_DATE.min().date()).days\nax = plot_history_alive(mbgnbd, t, sp_trans, 'ORDER_DATE', start_date=two_year_ago) #, start_date='2016-01-01'\nax.vlines(x=today, ymin=0, ymax=1.05, colors='#4C4C4C')\nax.hlines(y=0.8, xmin=two_year_ago, xmax=one_year_from_now, colors='#4C4C4C')\n\nax.set_xlim(two_year_ago, one_year_from_now) # sp_trans.ORDER_DATE.min()\nax.set_ylim(0, 1.05)\n\nplt.xticks(rotation=-90)\ntext(0.75, 0.1, p_alive_today, ha='center', va='center', transform=ax.transAxes)\n\nsns.despine()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicted Transactions with Time\nelog.columns = ['CUSTOMER_ID','date']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Get expected and actual repeated cumulative transactions.\n\nfrom lifetimes.utils import expected_cumulative_transactions\n\nt = (elog.date.max() - elog.date.min()).days\ndf = expected_cumulative_transactions(mbgnbd, elog, 'date', 'CUSTOMER_ID', t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf.tail()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Calibration period = 2016-09-04 to 2017-09-30\nfrom datetime import datetime\n\ncal = datetime.strptime('2018-06-30', '%Y-%m-%d')\n\nfrom lifetimes.plotting import plot_cumulative_transactions\nt = (elog.date.max() - elog.date.min()).days\nt_cal = (cal - elog.date.min()).days\nplot_cumulative_transactions(mbgnbd, elog, 'date', 'CUSTOMER_ID', t, t_cal, freq='D')\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n%%time \n\nfrom lifetimes.plotting import plot_incremental_transactions\nplot_incremental_transactions(mbgnbd, elog, 'date', 'CUSTOMER_ID', t, t_cal, freq='D')\nsns.despine()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_cal_holdout.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #  Reading the datasets \n# df2 = pd.read_csv('../input/brazilian-ecommerce/olist_customers_dataset.csv')\n# df6 = pd.read_csv('../input/brazilian-ecommerce/olist_geolocation_dataset.csv')\n# df4 = pd.read_csv('../input/brazilian-ecommerce/olist_order_items_dataset.csv')\n# df3 = pd.read_csv('../input/brazilian-ecommerce/olist_order_payments_dataset.csv')\n# df5 = pd.read_csv('../input/brazilian-ecommerce/olist_order_reviews_dataset.csv')\n# df1 = pd.read_csv('../input/brazilian-ecommerce/olist_orders_dataset.csv')\n# df7 = pd.read_csv('../input/brazilian-ecommerce/olist_products_dataset.csv')\n# df8 = pd.read_csv('../input/brazilian-ecommerce/olist_sellers_dataset.csv')\n# df9 = pd.read_csv('../input/brazilian-ecommerce/product_category_name_translation.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## To read the data from  transactions , \n### -- BTW cal is is for fitting the dataset and test is done on holdout (feature and target )"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Lifetimes function to read the transactions \nfrom lifetimes.datasets import load_transaction_data\nfrom lifetimes .utils import summary_data_from_transaction_data\n\ntransaction_data = load_transaction_data()\nprint(transaction_data)\n\nsummary = summary_data_from_transaction_data(transaction_data,'id','date',observation_period_end ='2018-09-28')\nprint(summary.head())\nmbgnbd.fit(summary_cal_holdout['frequency_cal'],summary_cal_holdout['recency_cal'], summary_cal_holdout['T_cal'])\nprint(mbgnbd)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BASED ON HISTORY ==== INDIVIDUAL "},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Customer predictions of future purchases\nt = 30 \nindividual  = summary_cal_holdout.iloc[20]\n#  expected purchases for each individual \nmbgnbd.predict(t,individual['frequency_cal'], individual['recency_cal'], individual['T_cal'])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transaction data features \n# frame = {recency , frequency , monetary 'CUSTOMER_ID', 'order_date', 'cumulative_transactions'}\n# recency = (features_data.groupby('CUSTOMER_ID')['date'])\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GAMMA GAMMA : Estimating customer lifetime value using the GAMMA-GAMMA model "},{"metadata":{"trusted":true},"cell_type":"code","source":"from lifetimes.datasets import load_cdnow_summary_data_with_monetary_value\nsummary_with_money_value = load_cdnow_summary_data_with_monetary_value()\nsummary_with_money_value.head()\nreturning_customers_summary = summary_with_money_value[summary_with_money_value['frequency']>0]\n\nprint(returning_customers_summary.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  this Gamma submodel  is based on the assumption that  a customer's monetary value is not based on the purchase frequency\n\nreturning_customers_summary[['monetary_value','frequency','recency','T']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## Gamma-Gamma submodel and predict the conditional, expected average lifetime value of our customers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submodel GammaGamma based on the assumption frequency and monetary value they are independent\nfrom lifetimes import GammaGammaFitter\nggf = GammaGammaFitter(penalizer_coef = 0)\nggf.fit(returning_customers_summary['frequency'], \n       returning_customers_summary['monetary_value'] )\nprint(ggf)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}